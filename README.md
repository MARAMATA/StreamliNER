# ğŸ·ï¸ NER Wolof - Reconnaissance d'EntitÃ©s NommÃ©es en Wolof

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![Transformers](https://img.shields.io/badge/ğŸ¤—-Transformers-yellow.svg)](https://huggingface.co/transformers/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-red.svg)](https://streamlit.io)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> Un systÃ¨me de reconnaissance d'entitÃ©s nommÃ©es (NER) spÃ©cialement entraÃ®nÃ© pour la langue wolof, utilisant des modÃ¨les Transformer state-of-the-art.

## ğŸŒŸ AperÃ§u

Ce projet propose un modÃ¨le de reconnaissance d'entitÃ©s nommÃ©es pour le **wolof**, une langue parlÃ©e par plus de 12 millions de personnes principalement au SÃ©nÃ©gal. Le modÃ¨le peut identifier et classifier automatiquement :

- ğŸ‘¤ **PER** - Personnes
- ğŸ¢ **ORG** - Organisations  
- ğŸ“ **LOC** - Lieux
- ğŸ“… **DATE** - Dates

## ğŸ¯ Performances

| MÃ©trique | Validation | Test |
|----------|------------|------|
| **F1 Score** | 79.85% | 78.28% |
| **PrÃ©cision** | 79.09% | 77.20% |
| **Rappel** | 80.62% | 79.38% |
| **Accuracy** | 98.11% | 97.96% |

## ğŸš€ DÃ©mo en Ligne

Une interface web interactive est disponible pour tester le modÃ¨le en temps rÃ©el :

```bash
streamlit run app.py
```

![Demo Screenshot](https://via.placeholder.com/800x400/4CAF50/white?text=Demo+Screenshot)

## ğŸ“¦ Installation

### PrÃ©requis

- Python 3.8+
- PyTorch
- Transformers
- Streamlit

### Installation rapide

```bash
# Cloner le repository
git clone https://github.com/votre-username/ner-wolof.git
cd ner-wolof

# CrÃ©er un environnement virtuel
python -m venv ner-env
source ner-env/bin/activate  # Linux/Mac
# ou
ner-env\Scripts\activate  # Windows

# Installer les dÃ©pendances
pip install -r requirements.txt
```

### DÃ©pendances

```txt
torch>=1.9.0
transformers>=4.20.0
datasets>=2.0.0
streamlit>=1.28.0
scikit-learn>=1.0.0
evaluate>=0.4.0
numpy>=1.21.0
pandas>=1.3.0
plotly>=5.0.0
requests>=2.25.0
beautifulsoup4>=4.9.0
```

## ğŸƒâ€â™‚ï¸ Utilisation Rapide

### Interface Web (RecommandÃ©)

```bash
streamlit run app.py
```

### API Python

```python
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

# Charger le modÃ¨le
model_path = "./ner-wolof-final-5epochs"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForTokenClassification.from_pretrained(model_path)

# CrÃ©er le pipeline
ner_pipeline = pipeline("ner", 
                       model=model, 
                       tokenizer=tokenizer,
                       aggregation_strategy="simple")

# Analyser un texte
text = "Aminata dem na ThiÃ¨s te mu jÃ«nd ci UCAD"
results = ner_pipeline(text)

for entity in results:
    print(f"{entity['word']} â†’ {entity['entity_group']} ({entity['score']:.3f})")
```

### Notebook Jupyter

Un notebook complet d'entraÃ®nement est disponible : [`NER WOLOF-1.ipynb`](NER%20WOLOF-1.ipynb)

## ğŸ”§ EntraÃ®nement du ModÃ¨le

### Dataset

Le modÃ¨le est entraÃ®nÃ© sur **MasakhaNER 2.0**, un dataset spÃ©cialisÃ© pour les langues africaines :

- **Train** : 4,593 exemples
- **Validation** : 656 exemples  
- **Test** : 1,312 exemples

### Architecture

- **ModÃ¨le de base** : `xlm-roberta-base`
- **OptimisÃ©** pour les langues peu dotÃ©es
- **Tokenisation** : SentencePiece (meilleure pour le wolof)

### Configuration d'entraÃ®nement

```python
training_args = TrainingArguments(
    num_train_epochs=5,
    learning_rate=2e-5,
    per_device_train_batch_size=2,
    weight_decay=0.01,
    warmup_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_f1"
)
```

## ğŸ“Š Exemples d'Utilisation

### Texte en Wolof

```python
# Exemple 1
text = "Ousmane Sonko ak Macky Sall naÃ±u togg ci Dakar"
# RÃ©sultat: Ousmane Sonko (PER), Macky Sall (PER), Dakar (LOC)

# Exemple 2  
text = "UNESCO ak gouvernement bu SÃ©nÃ©gal bokk ci projet bi"
# RÃ©sultat: UNESCO (ORG), SÃ©nÃ©gal (LOC)

# Exemple 3
text = "Meeting bi dina am ci janvier 2024"
# RÃ©sultat: janvier 2024 (DATE)
```

### Texte Mixte (Wolof-FranÃ§ais)

Le modÃ¨le gÃ¨re aussi les textes mixtes :

```python
text = "Ministre Ahmed Aidara wax na ne rÃ©union bi dina am ci juin"
# RÃ©sultat: Ahmed Aidara (PER), juin (DATE)
```

## ğŸ—ï¸ Structure du Projet

```
ner-wolof/
â”œâ”€â”€ ğŸ““ NER WOLOF-1.ipynb           # Notebook d'entraÃ®nement
â”œâ”€â”€ ğŸ¨ app.py                      # Interface Streamlit
â”œâ”€â”€ ğŸ¨ app_modern.py               # Interface moderne
â”œâ”€â”€ ğŸ“ ner-wolof-final-5epochs/    # ModÃ¨le entraÃ®nÃ©
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ label_mappings.json
â”‚   â”œâ”€â”€ evaluation_results.json
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ example_usage.py
â”œâ”€â”€ ğŸ“‹ requirements.txt            # DÃ©pendances
â”œâ”€â”€ ğŸ“„ README.md                   # Ce fichier
â””â”€â”€ ğŸ“¸ screenshots/                # Captures d'Ã©cran
```

## ğŸ”¬ MÃ©thodologie

### 1. PrÃ©paration des DonnÃ©es

- **Dataset** : MasakhaNER 2.0 (wolof)
- **PrÃ©processing** : Tokenisation optimisÃ©e
- **Gestion** des mots coupÃ©s (sous-mots)

### 2. Architecture du ModÃ¨le

```python
XLM-RoBERTa Base (125M paramÃ¨tres)
â†“
Token Classification Head (9 classes)
â†“
Labels: O, B-PER, I-PER, B-ORG, I-ORG, B-LOC, I-LOC, B-DATE, I-DATE
```

### 3. EntraÃ®nement

- **5 epochs** avec early stopping
- **Learning rate scheduling** avec warmup
- **Weight decay** pour Ã©viter l'overfitting
- **Ã‰valuation** Ã  chaque epoch

### 4. Optimisations

- **SÃ©lection automatique** du meilleur tokenizer
- **Gestion intelligente** des sous-mots
- **Post-traitement** pour fusionner les entitÃ©s fragmentÃ©es

## ğŸ“ˆ Analyse des RÃ©sultats

### Performance par Type d'EntitÃ©

| Type | PrÃ©cision | Rappel | F1-Score | Support |
|------|-----------|---------|----------|---------|
| **PER** | 80.11% | 91.89% | 85.60% | 456 |
| **LOC** | 91.22% | 79.48% | 84.95% | 536 |
| **ORG** | 79.06% | 79.28% | 79.17% | 362 |
| **DATE** | 81.16% | 66.27% | 72.96% | 169 |

### Matrice de Confusion

![Confusion Matrix](https://via.placeholder.com/400x300/2196F3/white?text=Confusion+Matrix)

## ğŸ”„ AmÃ©liorer le ModÃ¨le

### Ajout de DonnÃ©es

```python
# Ajouter vos propres donnÃ©es d'entraÃ®nement
from datasets import Dataset

custom_data = {
    "tokens": [["Nouveau", "texte", "en", "wolof"]],
    "ner_tags": [0, 0, 0, 0]  # O, O, O, O
}

custom_dataset = Dataset.from_dict(custom_data)
```

### Fine-tuning

```python
# Continuer l'entraÃ®nement sur vos donnÃ©es
trainer = Trainer(
    model=model,
    train_dataset=custom_dataset,
    # ... autres paramÃ¨tres
)

trainer.train()
```

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! Voici comment contribuer :

1. **Fork** le projet
2. **CrÃ©er** une branche (`git checkout -b feature/AmazingFeature`)
3. **Commit** vos changements (`git commit -m 'Add AmazingFeature'`)
4. **Push** vers la branche (`git push origin feature/AmazingFeature`)
5. **Ouvrir** une Pull Request

### Guidelines

- âœ… Suivre le style de code existant
- âœ… Ajouter des tests pour les nouvelles fonctionnalitÃ©s
- âœ… Mettre Ã  jour la documentation
- âœ… VÃ©rifier que tous les tests passent

## ğŸ“‹ TODO

- [ ] ğŸŒ DÃ©ploiement sur Hugging Face Hub
- [ ] ğŸ“± Application mobile
- [ ] ğŸ”Š Support audio (Speech-to-NER)
- [ ] ğŸ“š Dataset Ã©tendu avec plus d'exemples
- [ ] ğŸš€ ModÃ¨le plus rapide (DistilBERT)
- [ ] ğŸŒ Support multilingue (franÃ§ais-wolof)

## ğŸ› ProblÃ¨mes Connus

### RÃ©solution des Erreurs Communes

```bash
# Erreur de mÃ©moire sur CPU
export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0

# ProblÃ¨me de tokenisation
pip install transformers --upgrade

# Erreur Streamlit
streamlit cache clear
```

## ğŸ“š Ressources

### Documentation

- [Transformers Documentation](https://huggingface.co/transformers/)
- [MasakhaNER Paper](https://arxiv.org/abs/2103.11811)
- [XLM-RoBERTa Paper](https://arxiv.org/abs/1911.02116)

### Datasets

- [MasakhaNER 2.0](https://huggingface.co/datasets/masakhane/masakhaner2)
- [Wolof Wikipedia](https://wo.wikipedia.org/)

### ModÃ¨les Similaires

- [Davlan/bert-base-multilingual-cased-masakhaner](https://huggingface.co/Davlan/bert-base-multilingual-cased-masakhaner)
- [microsoft/mdeberta-v3-base](https://huggingface.co/microsoft/mdeberta-v3-base)

## ğŸ“„ License

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ™ Remerciements

- **MasakhaNE** pour le dataset MasakhaNER
- **Hugging Face** pour la bibliothÃ¨que Transformers
- **CommunautÃ© Wolof** pour le support linguistique
- **UniversitÃ© Cheikh Anta Diop** pour les ressources

## ğŸ“ Contact

- **Auteur** : Votre Nom
- **Email** : votre.email@example.com
- **LinkedIn** : [Votre Profil](https://linkedin.com/in/votre-profil)
- **Twitter** : [@VotreHandle](https://twitter.com/VotreHandle)

## ğŸ“Š Statistiques du Projet

![GitHub stars](https://img.shields.io/github/stars/votre-username/ner-wolof)
![GitHub forks](https://img.shields.io/github/forks/votre-username/ner-wolof)
![GitHub issues](https://img.shields.io/github/issues/votre-username/ner-wolof)
![GitHub last commit](https://img.shields.io/github/last-commit/votre-username/ner-wolof)

---

<div align="center">
  <strong>ğŸŒ Fait avec â¤ï¸ pour la langue wolof</strong>
  <br>
  <sub>Contribuer Ã  la prÃ©servation et Ã  la valorisation des langues africaines</sub>
</div>
